{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObdoTnPSF0R6PnaYk2p3Ci",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJMortensonWarwick/large_scale_data_for_research/blob/main/web_scraping_with_beautiful_soup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Web Scraping with requests and BeautifulSoup\n",
        "In this notebook we will practice web scraping and information extraction using the packages requests and BeautifulSoup and the world renowned website of the [University of Warwick](www.warwick.ac.uk).\n",
        "\n",
        "To begin, we need to check the install of the relevant packages."
      ],
      "metadata": {
        "id": "4XhARiKna3uo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST4yYXr0a2V8",
        "outputId": "5d3430c0-b5fa-49fd-f5e6-77fe939ea99c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also setup a header variable - this basically tells the website this request is from a normal browser-type agent:"
      ],
      "metadata": {
        "id": "qtBPAf7xbUZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from requests import get\n",
        "import pandas as pd\n",
        "\n",
        "headers = ({'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'})"
      ],
      "metadata": {
        "id": "-LbUCtX5bVGQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here we can specify our website and make a GET request (as with an API). We can print the start of the response to check we have been able to access the site content."
      ],
      "metadata": {
        "id": "SAmOB--tb4eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warwick_web = \"https://warwick.ac.uk\"\n",
        "response = get(warwick_web, headers=headers)\n",
        "\n",
        "print(response.text[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsb3Snyub75Q",
        "outputId": "1d9c7f6e-4ce0-4011-a231-9482e5cc6db1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!doctype html>\n",
            "<html lang=\"en-GB\" class=\"no-js\">\n",
            "    <head>\n",
            "        <base href=\"https://warwick.ac.uk/\">\n",
            "\n",
            "        <meta charset=\"utf-8\">\n",
            "<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\n",
            "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
            "\n",
            "<title>Welcome to the University of Warwick</title>\n",
            "\n",
            "<meta name=\"description\" content=\"University of Warwick website\">\n",
            "<meta name=\"keywords\" content=\"University of Warwick, university, warwick, uk universities, warwick university, uk, uni\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have scraped some website content lets see if we can find anything useful in it (by parsing the content with BeautifulSoup). Let's start by extracting all the h1 tags (first-level headers):"
      ],
      "metadata": {
        "id": "5mFJvD5jcWJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warwick_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "warwick_h1 = warwick_soup.find_all('h1')\n",
        "print(warwick_h1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHVjXR9kccUM",
        "outputId": "0842cd78-ac43-459e-d9fb-62250f11769c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<h1>\n",
            "                            \n",
            "                                The University of Warwick\n",
            "                            \n",
            "                            \n",
            "                        </h1>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do something similar for h3 tags and extract only the text rather than the HTML tags:"
      ],
      "metadata": {
        "id": "Uqs09rUSckPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warwick_tag = warwick_soup.find_all('h3')\n",
        "\n",
        "for each in warwick_tag:\n",
        "    print(str(each.get_text()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XGcyNCzcZIJ",
        "outputId": "5e463c2c-04ef-440c-dce4-40f35b175a0a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Postgraduate Study 2024\n",
            "Research Stories\n",
            "National Centre for Research Culture\n",
            "Knowledge Exchange\n",
            "Warwick has been awarded Gold in all categories of the government's latest Teaching Excellence Framework (TEF) rankings\n",
            "Labour leader sees University of Warwick’s industrial impact first hand\n",
            "Warwick opens new Venice home as part of record £100m investment in the arts\n",
            "University of Warwick recognised as international centre of research excellence by leading experts\n",
            "Connect with us\n",
            "Talk to us\n",
            "Find us\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will loop between two web pages (the suffixes list below) and extract the h1 headings, the meta title and the meta description. As the website stores content inside containers, we will need to search inside these, again via a loop. Finally you will note we use the time module in order to make the script wait - via sleep( ). This is good practice when scraping websites as a script can make a lot of requests very quickly and overload the website server. We sleep for a random time just for fun. We also time the whole process using the Notebook function %%time."
      ],
      "metadata": {
        "id": "fb7HW3iWdOp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "from random import randint\n",
        "from time import sleep\n",
        "\n",
        "# setting up the lists/dictionary that will form our dataframe with all the results\n",
        "titles = []\n",
        "metatitle = []\n",
        "title_dict = {} # empty dictionary\n",
        "\n",
        "uri = 'https://warwick.ac.uk/research/'\n",
        "suffixes = ['ref', 'partnerships'] # add the rest in here\n",
        "\n",
        "for suffix in suffixes: # loop through the suffixes list\n",
        "\n",
        "    h1heading = [] # new list or empty the list to start again\n",
        "    metatitle = []\n",
        "    metadesc = []\n",
        "    warwick_webscrape = uri + suffix # concatenate the URL and the suffix in the current loop\n",
        "    r = get(warwick_webscrape, headers=headers)\n",
        "    page_html = BeautifulSoup(r.text, 'html.parser')\n",
        "    warwick_webpage = page_html.find_all('html')\n",
        "\n",
        "    if warwick_webpage != []:\n",
        "        for container in warwick_webpage:\n",
        "\n",
        "            # page title\n",
        "            meta_title = page_html.find(\"meta\", property=\"og:title\")\n",
        "            metatitle.append(meta_title)\n",
        "\n",
        "            # meta description\n",
        "            meta_desc = page_html.find(\"meta\", property=\"og:description\")\n",
        "            metadesc.append(meta_desc)\n",
        "\n",
        "            # H1\n",
        "            h1name = container.find_all('h1')[0].text\n",
        "            h1heading.append(h1name)\n",
        "\n",
        "\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    title_dict[suffix] = h1heading, metatitle, metadesc # add to the dictionary the suffix and the list of h1's\n",
        "\n",
        "\n",
        "    sleep(randint(1,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz7dhZx8dOZL",
        "outputId": "cf67038f-3509-4494-bb8b-b59093674650"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 258 ms, sys: 8.26 ms, total: 267 ms\n",
            "Wall time: 5.38 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now inspect our data:"
      ],
      "metadata": {
        "id": "ON6JsnkTd2mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(title_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BowkNJvgd6_5",
        "outputId": "bf724198-fecc-4559-aa04-1a38de68156b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ref': (['\\n\\nResearch\\n\\n'], [<meta content=\"92% of our research is world-leading or internationally excellent.\" data-user-meta=\"true\" property=\"og:title\"/>], [<meta content=\"Our results in REF 2021 demonstrate the world class quality of our research, our approach, and most importantly, our people.\" data-user-meta=\"true\" property=\"og:description\"/>]), 'partnerships': (['\\n\\nResearch\\n\\n'], [None], [None])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To finish off our work we will add our data to a pandas DataFrame and export as a CSV."
      ],
      "metadata": {
        "id": "6fw4ZYMveAto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['Title'] # columns in our output file\n",
        "\n",
        "warwickcsv = pd.DataFrame({'Title': title_dict})[cols]\n",
        "warwickcsv.to_csv('warwick_scrape.csv') # the name of our file\n",
        "\n",
        "# download a file to browser downloads\n",
        "from google.colab import files\n",
        "files.download(\"/content/warwick_scrape.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "XiWY4erEeBtm",
        "outputId": "79f6253c-9ecb-402b-b401-e9dacdd5bc54"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_998a80c5-464c-4aff-a5ac-5d11b0fe448e\", \"warwick_scrape.csv\", 426)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}